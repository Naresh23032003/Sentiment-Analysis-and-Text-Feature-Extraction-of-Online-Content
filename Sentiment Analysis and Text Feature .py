# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12_gQW8XxryTGlNNqc3oZUAB_n6CD6sJu
"""

# Commented out IPython magic to ensure Python compatibility.
# Install BeautifulSoup library
!pip install bs4

# Import necessary libraries
import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import re

# Uncomment the below 2 lines to mount Google Drive to access files if Colab is used to run the script
#from google.colab import drive
#drive.mount('/content/drive')

# Set the directory path
dir = "/content/drive/MyDrive/BC" #Edit it to the directory in which files are stored
# %cd "/content/drive/MyDrive/BC" #Edit it to the directory in which files are stored
os.system("mkdir TextFile") #To store the text
# %cd "/content/drive/MyDrive/BC"

# Download NLTK resources required for text processing
nltk.download('punkt')
nltk.download('stopwords')

# Read input data from Excel
df = pd.read_excel('Input.xlsx')

# Loop through the input data and extract content from URLs
for index, row in df.iterrows():
    url = row['URL']
    url_id = row['URL_ID']
    try:
        # Get content from URL using requests and BeautifulSoup
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
    except:
        print("Error while parsing file {}".format(url_id))
        continue
    try:
        # Extract title from the HTML content
        title = soup.find('h1').get_text()
    except:
        print("Error title not found so using filename of {}".format(url_id))
        title = str(url_id)
    content = ""
    # Extract text content from paragraphs
    for txt in soup.find_all('p'):
        content += txt.get_text()

    # Write title and content to a text file
    file = dir + '/TextFile/' + str(url_id) + '.txt'
    with open(file, 'w') as f:
        f.write(title + '\n' + content)

# Define sets for stopwords, positive words, and negative words
s_words = set()
p = set()
n = set()

# Loop through directories to gather stopwords, positive, and negative words
for files in os.listdir(dir + "/StopWords"):
    with open(os.path.join(dir + "/StopWords" , files), 'r', encoding='ISO-8859-1') as f:
        s_words.update(set(f.read().splitlines()))

for file in os.listdir(dir + "/MasterDictionary"):
    if file =='positive-words.txt':
        with open(os.path.join(dir + "/MasterDictionary", file), 'r', encoding='ISO-8859-1') as f:
            p.update(f.read().splitlines())
    elif file =='negative-words.txt':
        with open(os.path.join(dir + "/MasterDictionary", file), 'r', encoding='ISO-8859-1') as f:
            n.update(f.read().splitlines())

# Create an empty list to store processed documents
docs = []

# Process text files and tokenize words
for text_file in os.listdir(dir + "/TextFile"):
    with open(os.path.join(dir + "/TextFile", text_file), 'r') as f:
        text = f.read()
        words = word_tokenize(text)
        # Filter out stopwords from words
        filteredtext = [word for word in words if word.lower() not in s_words]
        docs.append(filteredtext)

# Initialize lists to store positive and negative words, and their scores
pos = []
neg = []
p_score = []
n_score = []
score_pol = []
score_sub = []

# Iterate through the list of documents
for index in range(len(docs)):
    # Filter positive and negative words from each document
    pos.append([word for word in docs[index] if word.lower() in p])
    neg.append([word for word in docs[index] if word.lower() in n])

    # Calculate positive and negative scores for each document
    p_score.append(len(pos[index]))
    n_score.append(len(neg[index]))

    # Calculate polarity and subjectivity scores for each document
    score_pol.append((p_score[index] - n_score[index]) / ((p_score[index] + n_score[index]) + 0.000001))
    score_sub.append((p_score[index] + n_score[index]) / ((len(docs[index])) + 0.000001))

# Calculate average sentence lengths, percent of complex words, etc.
avg_sentence_lengths = []
percent_complex_words = []
fog_indexes = []
complex_word_counts = []
word_count = []
avg_syllable_counts = []
pronoun_count = []
average_word_lengths = []

# Set stopwords and personal pronouns for English language
stop_words = set(stopwords.words("english"))
pronouns = ["I", "we", "my", "ours", "us"]

# Process each text file to calculate various text metrics
for filename in os.listdir(dir + "/TextFile"):
    with open(os.path.join(dir + "/TextFile", filename), "r") as f:
        text = f.read()
        text = re.sub(r"[^\w\s.]", "", text)
        sentences = text.split(".")
        num_sentences = len(sentences)
        words = [word for word in text.split() if word.lower() not in stop_words]
        num_words = len(words)

        # Calculate complex words, syllable count, average sentence length, personal pronoun count, word count, average word length.
        length = sum(len(word) for word in words)
        average_word_length = length / len(words) if len(words) > 0 else 0
        complex_words = [word for word in words if len(re.findall('[aeiou]', word.lower())) > 2]
        syllable_count = sum([sum(1 for letter in word if letter.lower() in 'aeiou') for word in words])
        avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0
        percent_complex_word = len(complex_words) / num_words if num_words > 0 else 0
        fog_index = 0.4 * (avg_sentence_length + percent_complex_word)
        count = sum(len(re.findall(r"\b" + pronoun + r"\b", text)) for pronoun in pronouns)

        # Append calculated metrics to respective lists
        avg_sentence_lengths.append(avg_sentence_length)
        percent_complex_words.append(percent_complex_word)
        fog_indexes.append(fog_index)
        complex_word_counts.append(len(complex_words))
        word_count.append(len(words))
        avg_syllable_counts.append(syllable_count / num_words if num_words > 0 else 0)
        pronoun_count.append(count)
        average_word_lengths.append(average_word_length)

# Prepare output DataFrame for storing analyzed data
output = pd.read_excel('Output Data Structure.xlsx')

# List of variables to be added to the output DataFrame
# Add all the calculated metrics here
vars = [
  p_score, n_score, score_pol, score_sub, avg_sentence_lengths,percent_complex_words,
  fog_indexes, avg_sentence_lengths, complex_word_counts, word_count, avg_syllable_counts,
  pronoun_count, average_word_lengths
]

# Loop through output DataFrame and assign values to respective columns
for i in range(len(vars)):
  output.iloc[:,i+2] = vars[i]

# Write the analyzed data to an output CSV file
output.to_excel('Output.xlsx')